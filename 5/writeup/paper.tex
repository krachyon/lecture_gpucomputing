\section{Paper review: <The GPU Computing Era>}
This paper by John Nickolls and William J. Dally from NVIDIA summarizes the developments in the field of GPUs which have taken place from 1997 up to the presentation of the Fermi infrastructure in 2010 and have let the GPU from being a pure graphics processing unit to a powerful parallel computing platform. Main aspects are the ongoing advances in transistor densities, the growing use of flexible and programmable components like streaming multiprocessors, and especially the introduction of GPU computing programming languages like CUDA in 2007. The advent of specialized cards designed only for parallel computing instead of graphical processing finalizes the transition.\\
One of the strong points of the CUDA architecture is its scalability: the developer does not have to optimize for a given multiprocessor count, since thread blocks and the threads themselves deliver independent coarse- and fine-grained data parallelism with little effort. The essential building block of an programmable GPU is the streaming multiprocessor, built upon the single-instruction-multiple-thread (SIMT) principle, essential in further enabling scalable parallel computing. The paper proposes the use of combined CPU/GPU systems to utilize the strengths of both architectures, as almost all problems contain both serial and parallelizable code, where a CPU or GPU on their own are limited due to their structure.\\
The paper gives a good elementary introduction into a broad spectrum of topics related to GPU computing and its development over time. Written almost 10 years ago, many predictions still hold today: GPU computing has gained popularity in the last decade. The increased focus on Machine Learning, which are easily parallelizable applications, is only one of the latest trends further boosting GPU computing, with no end in sight.