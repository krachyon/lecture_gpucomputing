\documentclass[11pt,twoside,a4paper]{scrartcl}
\usepackage{amsmath}
\usepackage{fourier}
\usepackage{graphicx}
\usepackage[left=3.00cm, right=3.00cm, top=3.00cm, bottom=3.00cm]{geometry}

\usepackage[utf8x]{inputenc}
\usepackage[ngerman,english]{babel}

\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{pygmentex}

\setlength{\parskip}{0.5em plus0.2em minus0.2em}
\setlength{\parindent}{1em}

\title{GPU computing exercise 7}
\date{\today}
\author{Sebastian Me√ülinger, Niklas Euler}

\begin{document}
\maketitle

\section{used code}

All performance numbers are derived from the kernel runtime without setup/copy steps.
The CUDA parts where compiled into a shared library, benchmark results where obtained with the \href{https://github.com/google/benchmark}{benchmark} library which automatically adapts the iteration count of runs until a stable result is produced.
Two variants were implemented: The naive version that accesses an array of structs from global memory and a version making use of shared memory and loop unrolling for separate arrays for position, mass and velocity.

Both implementations are not optimal in the sense that they do not aggregate adjacent half steps for the velocity integration.

\section{results}
\subsection{alignment}

To test the effect of alignment, the naive AOS version was executed two times, with and without alignment to 32 bytes. The results on performance are plotted in \ref{align}. There's a clear benefit from aligning the data to 32 bytes instead of the packed alignment of 28. This is  most likely due to simplifying the generated instruction stream and enabling coalescing of memory accesses into a more uniform pattern. 

The result is a bit overshadowed by the fact that the naive version performs best at very low thread\_per\_block values. The highest value is actually reached by the unaligned variant at 1 thread per block.
Additionally, re-ordering the fields might have an impact depending on the way subsequent fields are accessed.

\begin{pygmented}[lang=cpp]
    struct __align__(32) Body{
        float3 pos;
        float mass;
        float3 velocity;
    };    
\end{pygmented}
\begin{figure}[h!]
    \includegraphics[width=1.0\textwidth]{../results/alignment.pdf}
    \caption{}
    \label{align}
\end{figure}

\subsection{performance of naive implementation compared}

The shared variant outperforms the naive variant consistently across the range of tested problem sizes and thread\_per\_block settings. It is however very sensitive to the choice of thread block size. A range between 1 and 700 threads per block was tested, averaging over all block sizes the naive version outperforms the shared version with a relatively consistent but small margin. For the optimal choice of kernel parameters the gap can be more than double. 

The very fast results between $256 \le N \le 640$ seem to be outliers, either in the sense that we missed something crucial in the implementation and only reached decent performance in these cases or they where abnormally fast for similar reasons.

Most problem sizes have a set of very clear optimum values for the block size.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{../results/nvsflops1.pdf}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{../results/nvsflops2.pdf}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{../results/threadcount_naive.pdf}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{../results/threadcount_shared.pdf}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{../results/heatmap_naive.pdf}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{../results/heatmap_shared.pdf}
\end{figure}

\end{document}